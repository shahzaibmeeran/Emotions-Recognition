{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa15e58-6812-4870-8563-2042c7772024",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df5edb-682d-47b7-b716-132e85a6b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification, TFBertForSequenceClassification, AutoTokenizer, AdamW\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf8345e-00df-468f-b3bc-1f92bac89b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['/kaggle/input/goemotions-dataset/goemotions_1.csv', '/kaggle/input/goemotions-dataset2/goemotions_2.csv', '/kaggle/input/goemotions-dataset3/goemotions_3.csv']\n",
    "\n",
    "merged_df = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(file_path)\n",
    "        for file_path in file_paths\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958bd69-c657-47da-aaac-bd3e2b420067",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07240a1b-8519-4d3c-a869-e25215d5b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.drop(\n",
    "    [\"id\",\"author\",\"subreddit\",\"link_id\",\"parent_id\",\"created_utc\",\"rater_id\",\"example_very_unclear\"],\n",
    "    axis=1,\n",
    ")\n",
    "df = df.dropna(axis=0)\n",
    "dict_keys = [key for key in df.columns if key != \"text\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38da0e-4f00-416d-abaa-5df62f5b2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf337f21-2e2b-4635-9d29-11c00a7638d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns[df.columns.get_loc('anger'):df.columns.get_loc('neutral')+1]\n",
    "\n",
    "# Veriyi eritme işlemi\n",
    "df = df.melt(id_vars=['text'], value_vars=cols, value_name='emotion')\n",
    "\n",
    "# Duygusal değeri 1 olan verileri seçin\n",
    "df = df[df['emotion'] == 1]\n",
    "\n",
    "# Duygu etiketlerini düzenleme\n",
    "emotion_dict = {\n",
    "    'annoyance': 'anger',\n",
    "    'excitement': 'joy',\n",
    "    'love': 'joy',\n",
    "    'disappointment': 'sadness',\n",
    "    'grief': 'sadness',\n",
    "    'remorse': 'sadness',\n",
    "    'embarrassment': 'sadness',\n",
    "    'curiosity': 'surprise',\n",
    "    'confusion': 'surprise'\n",
    "}\n",
    "\n",
    "df['variable'] = df['variable'].apply(lambda x: emotion_dict.get(x, x))\n",
    "\n",
    "# Kullanılmayan sınıfları kaldırma\n",
    "unused_classes = ['admiration', 'amusement', 'approval', 'caring', 'desire', 'disapproval', 'disgust', 'fear', 'gratitude', 'nervousness', 'optimism', 'pride', 'realization', 'relief']\n",
    "df = df[~df['variable'].isin(unused_classes)]\n",
    "\n",
    "# 'emotion' sütununu 'emotion_label' olarak yeniden adlandırma\n",
    "df.rename(columns={'variable': 'emotion_label'}, inplace=True)\n",
    "\n",
    "# Duygu gruplarını sayma\n",
    "emotion_counts = df.groupby('emotion_label').size()\n",
    "\n",
    "# Sonucu yazdırma\n",
    "print(df.head(10))\n",
    "print(emotion_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20428ed-d438-43a2-819f-9a618d534798",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebedc30-dd04-47c6-bcbe-d8cdfc64f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff975c6-1c1f-4f5b-8db9-707d1a3ad246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38696750-c53a-44fa-a176-a22d725181a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
    "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n",
    "\n",
    "punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n",
    "                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n",
    "                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n",
    "\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n",
    "                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n",
    "                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n",
    "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n",
    "                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
    "                'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8745f0-3ba2-4041-87a6-7d1c20b54bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r'\\:(.*?)\\:','',text)\n",
    "    text = str(text).lower()    # Making Text Lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    #The next 2 lines remove html text\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    '''Clean contraction using contraction mapping'''    \n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    for word in mapping.keys():\n",
    "        if \"\"+word+\"\" in text:\n",
    "            text = text.replace(\"\"+word+\"\", \"\"+mapping[word]+\"\")\n",
    "    #Remove Punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    '''Cleans special characters present(if any)'''   \n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  \n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    '''Corrects common spelling errors'''   \n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def remove_space(text):\n",
    "    '''Removes awkward spaces'''   \n",
    "    #Removes awkward spaces \n",
    "    text = text.strip()\n",
    "    text = text.split()\n",
    "    return \" \".join(text)\n",
    "\n",
    "def text_preprocessing_pipeline(text):\n",
    "    '''Cleaning and parsing the text.'''\n",
    "    text = clean_text(text)\n",
    "    text = clean_contractions(text, contraction_mapping)\n",
    "    text = clean_special_chars(text, punct, punct_mapping)\n",
    "    text = correct_spelling(text, mispell_dict)\n",
    "    text = remove_space(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5a601-191a-4956-9ae4-7f1be20c5889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
    "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n",
    "\n",
    "punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n",
    "                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n",
    "                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n",
    "\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n",
    "                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n",
    "                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n",
    "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n",
    "                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
    "                'demonetisation': 'demonetization'}\n",
    "\n",
    "def decontracted(df, column):\n",
    "    def expand_contractions(phrase):\n",
    "        for contraction, expansion in contraction_mapping.items():\n",
    "            phrase = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expansion, phrase)\n",
    "        return phrase\n",
    "    \n",
    "    def preprocess_text(text):\n",
    "        text = text.lower()  # Küçük harfe çevirme\n",
    "        text = text.replace(\"'\", \"\")  # Tek tırnak işaretlerini kaldırma\n",
    "        text = re.sub('[^\\w\\s]', '', text)  # Noktalama işaretlerini kaldırma\n",
    "        \n",
    "        for punct, replacement in punct_mapping.items():\n",
    "            text = text.replace(punct, replacement)\n",
    "        \n",
    "        for misspelled, correct in mispell_dict.items():\n",
    "            text = text.replace(misspelled, correct)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    df[column] = df[column].apply(expand_contractions)\n",
    "    df[column] = df[column].apply(preprocess_text)\n",
    "    \n",
    "    return df\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09be7e-4e1b-492c-973e-6486017cc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(subset='text')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ed319-3f00-44e4-a5d9-1cd19bc545bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='text', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be19d02-fea1-487e-929b-17c908822acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85625659-a19c-4b7c-9e6e-607f6578b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c399d8-250b-42b4-9922-f9b941e657a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['emotion_label'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2998e7-2ecc-4997-a4dc-cdfe99205c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_counts = df.groupby('emotion_label').size()\n",
    "\n",
    "plt.bar(emotion_counts.index, emotion_counts.values)\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Emotion Counts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837b11b-b491-4d39-b786-53d269e10918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = ' '.join(df['text'])\n",
    "\n",
    "# Kelime bulutunu oluşturma\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Emotion Word Cloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0bf02b-d785-4375-917d-a0a68520b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = wordcloud.process_text(text)\n",
    "\n",
    "for word, frequency in sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)[:100]:\n",
    "    print(word, frequency) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591f16a-938b-4767-a906-f0d9907f391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "emoji_rows = df[df[\"text\"].apply(lambda x: any(emoji.is_emoji(char) for char in x))]\n",
    "emoji_rows = emoji_rows.head(20)\n",
    "# Sonuçları yazdırma\n",
    "print(\"Emojilerin olduğu satırlar:\")\n",
    "for index, row in emoji_rows.iterrows():\n",
    "    print(\"Satır:\", index)\n",
    "    print(\"Metin:\", row[\"text\"])\n",
    "    print(\"Duygu:\", row[\"emotion_label\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825ce08-c2a8-4e60-8231-4cdbc7b719de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(df,col):\n",
    "  emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "  \n",
    "  df[\"text\"] = df[col].str.replace(emoji_pattern, '',regex=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a3686-0af6-49f1-af10-046fb5720c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_emojis(df, \"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796912b4-8c51-45b8-9a84-14d4faa32433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b19f6-7f44-47e2-b2a4-6d82d7b90333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r'\\[.*?\\]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8da090-3062-4ecf-8af8-c83edd035707",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee5d52-4334-465a-93d5-ef0d92789d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_rows = df[df[\"text\"].str.contains(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", regex=True)]\n",
    "\n",
    "# Sonuçları yazdırma\n",
    "print(\"Eşleşen satırlar:\")\n",
    "for index, row in matching_rows.iterrows():\n",
    "    print(\"Satır:\", index)\n",
    "    print(\"Metin:\", row[\"text\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461aa75b-0ee3-4798-986c-66f06d9d7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_numbers = df[df[\"text\"].str.contains(r'\\d', regex=True)]\n",
    "has_numbers = has_numbers.head(20)\n",
    "# Sonuçları yazdırma\n",
    "print(\"Sayıları içeren satırlar:\")\n",
    "for index, row in has_numbers.iterrows():\n",
    "    print(\"Satır:\", index)\n",
    "    print(\"Metin:\", row[\"text\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee7213-5662-469c-9b69-4de3e3f44365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].str.replace(r'\\d+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e6850-0e8f-4bc4-a154-ca4467d0b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].replace(r'[^\\w\\s\\'.;]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9eaafe-7e5e-4aad-ab60-5323e8eec55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d2201-c64b-4723-9307-cb0b2bfbdbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: ' '.join(word for word in x.split() if len(word) > 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b90980-6a46-454b-b3e9-609f6270fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fb07a-ce30-4dbf-8f59-f4dbb9545366",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7466d-c1ff-4c93-8758-645b66eeea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119c931-4f9b-4cb6-be01-d1aebffa8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306fc898-613d-407a-b72d-d79bfe494bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = df['emotion_label'].unique()\n",
    "label2id = {value: index for index, value in enumerate(label_list)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3cae8-9886-4f3b-aeeb-ab65b69ff0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63795e1b-8548-41c3-b3e5-0bfc87491365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f44e3-1280-4722-afe8-d80e4ec91cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c106c1-ac3f-4339-bee0-30fc90f7df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = tokenizer.batch_encode_plus(df['text'].tolist(), truncation=True, padding=True, return_attention_mask=True, return_tensors='tf')\n",
    "labels = [label2id[label] for label in df[\"emotion_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26a5c5-6283-42e5-a358-902170873351",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((dict(encoded_data), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ba95d-f3c3-4f15-a7f7-407cf3646bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"emotion\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb0cac-ef17-44e7-987f-c71809a2b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba9206-a7c0-4e89-b1d6-9392d1641daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"emotion_label\"])\n",
    "y = df[\"emotion_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a6f607-5a80-409d-8853-49c39b4f563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9cbe3-58b9-4e80-9a53-f6221ff47f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc90f9-626b-4b1c-a8d9-362fb2dcf187",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f924d-4537-4d50-93c0-0917155848f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe89a055-1e0c-4c56-88ba-47b52823f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_counts = np.unique(y_train, return_counts=True)\n",
    "train_percentages = train_counts / len(y_train) * 100\n",
    "\n",
    "# Calculate the percentage of each label in the test set\n",
    "test_labels, test_counts = np.unique(y_test, return_counts=True)\n",
    "test_percentages = test_counts / len(y_test) * 100\n",
    "\n",
    "# Create a bar chart to compare the label distribution in the training and test sets\n",
    "labels = y.unique()\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, train_percentages, width, label='Training set')\n",
    "rects2 = ax.bar(x + width/2, test_percentages, width, label='Test set')\n",
    "\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Add the percentage above each bar\n",
    "for i, rect in enumerate(rects1):\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2.0, 1.05 * rect.get_height(),\n",
    "            f'{train_percentages[i]:.2f}%', ha='center', va='bottom')\n",
    "for i, rect in enumerate(rects2):\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2.0, 1.05 * rect.get_height(),\n",
    "            f'{test_percentages[i]:.2f}%', ha='center', va='bottom')\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64760eda-606d-4971-8564-4acf3063079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(X_train, columns=[\"text\"])\n",
    "train_df['emotion_label'] = y_train\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "val_df = pd.DataFrame(X_val, columns=[\"text\"])\n",
    "val_df['emotion_label'] = y_val\n",
    "val_df = val_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(X_test, columns=[\"text\"])\n",
    "test_df['emotion_label'] = y_test\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ae498-92f7-4a55-9393-e249eb790b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encode = tokenizer.batch_encode_plus(train_df['text'].tolist(), truncation=True, padding=True, return_attention_mask=True, return_tensors='tf')\n",
    "train_labels = [label2id[label] for label in train_df[\"emotion_label\"]]\n",
    "\n",
    "val_encode = tokenizer.batch_encode_plus(val_df['text'].tolist(), truncation=True, padding=True, return_attention_mask=True, return_tensors='tf')\n",
    "val_labels = [label2id[label] for label in val_df[\"emotion_label\"]]\n",
    "\n",
    "test_encode = tokenizer.batch_encode_plus(test_df['text'].tolist(), truncation=True, padding=True, return_attention_mask=True, return_tensors='tf')\n",
    "test_labels = [label2id[label] for label in test_df[\"emotion_label\"]]\n",
    "\n",
    "\n",
    "train_dataset =  tf.data.Dataset.from_tensor_slices((dict(train_encode), train_labels)).batch(32)\n",
    "val_dataset =  tf.data.Dataset.from_tensor_slices((dict(val_encode), val_labels)).batch(32)\n",
    "test_dataset =  tf.data.Dataset.from_tensor_slices((dict(test_encode), test_labels)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe1240-9d55-4e6d-8ed1-8c174ff145c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label2id)\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5c46b-b8e7-4e27-afda-ad96033687d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b584249-ab69-44f7-a19b-463851811cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6da1f5-25d2-4145-bd64-835cf00feb30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20128c34-5eb3-48e9-bfde-c6a5fb8d0e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f17fe-f204-45d0-91fc-05f28dba1d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9661c-be6b-402f-9300-bed4df78cf10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ddf8c4-938e-466b-ade2-3db8ffdb5515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503cfa4-73ee-4b41-b5b3-e144bab1a5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
